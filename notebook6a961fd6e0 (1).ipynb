{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T08:50:13.046653Z","iopub.execute_input":"2025-12-02T08:50:13.046913Z","iopub.status.idle":"2025-12-02T08:50:14.645910Z","shell.execute_reply.started":"2025-12-02T08:50:13.046880Z","shell.execute_reply":"2025-12-02T08:50:14.645045Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e11/sample_submission.csv\n/kaggle/input/playground-series-s5e11/train.csv\n/kaggle/input/playground-series-s5e11/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\nprint(\"\\nTrain columns:\", train.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T08:50:14.725948Z","iopub.execute_input":"2025-12-02T08:50:14.726173Z","iopub.status.idle":"2025-12-02T08:50:17.962296Z","shell.execute_reply.started":"2025-12-02T08:50:14.726153Z","shell.execute_reply":"2025-12-02T08:50:17.961600Z"}},"outputs":[{"name":"stdout","text":"Train shape: (593994, 13)\nTest shape: (254569, 12)\n\nTrain columns: ['id', 'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade', 'loan_paid_back']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:52:14.275122Z","iopub.execute_input":"2025-11-08T11:52:14.275474Z","iopub.status.idle":"2025-11-08T11:52:14.308641Z","shell.execute_reply.started":"2025-11-08T11:52:14.275451Z","shell.execute_reply":"2025-11-08T11:52:14.307652Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n0   0       29367.99                 0.084           736      2528.42   \n1   1       22108.02                 0.166           636      4593.10   \n2   2       49566.20                 0.097           694     17005.15   \n3   3       46858.25                 0.065           533      4682.48   \n4   4       25496.70                 0.053           665     12184.43   \n\n   interest_rate  gender marital_status education_level employment_status  \\\n0          13.67  Female         Single     High School     Self-employed   \n1          12.92    Male        Married        Master's          Employed   \n2           9.76    Male         Single     High School          Employed   \n3          16.10  Female         Single     High School          Employed   \n4          10.21    Male        Married     High School          Employed   \n\n         loan_purpose grade_subgrade  loan_paid_back  \n0               Other             C3             1.0  \n1  Debt consolidation             D3             0.0  \n2  Debt consolidation             C5             1.0  \n3  Debt consolidation             F1             1.0  \n4               Other             D1             1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>annual_income</th>\n      <th>debt_to_income_ratio</th>\n      <th>credit_score</th>\n      <th>loan_amount</th>\n      <th>interest_rate</th>\n      <th>gender</th>\n      <th>marital_status</th>\n      <th>education_level</th>\n      <th>employment_status</th>\n      <th>loan_purpose</th>\n      <th>grade_subgrade</th>\n      <th>loan_paid_back</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>29367.99</td>\n      <td>0.084</td>\n      <td>736</td>\n      <td>2528.42</td>\n      <td>13.67</td>\n      <td>Female</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Self-employed</td>\n      <td>Other</td>\n      <td>C3</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>22108.02</td>\n      <td>0.166</td>\n      <td>636</td>\n      <td>4593.10</td>\n      <td>12.92</td>\n      <td>Male</td>\n      <td>Married</td>\n      <td>Master's</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>D3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>49566.20</td>\n      <td>0.097</td>\n      <td>694</td>\n      <td>17005.15</td>\n      <td>9.76</td>\n      <td>Male</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>C5</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>46858.25</td>\n      <td>0.065</td>\n      <td>533</td>\n      <td>4682.48</td>\n      <td>16.10</td>\n      <td>Female</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>F1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>25496.70</td>\n      <td>0.053</td>\n      <td>665</td>\n      <td>12184.43</td>\n      <td>10.21</td>\n      <td>Male</td>\n      <td>Married</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Other</td>\n      <td>D1</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Check target distribution\nprint(\"=== TARGET DISTRIBUTION ===\")\nprint(train['loan_paid_back'].value_counts(normalize=True))\n\n# Check data types\nprint(\"\\n=== DATA TYPES ===\")\nprint(train.dtypes)\n\n# Check for missing values\nprint(\"\\n=== MISSING VALUES ===\")\nprint(\"Train missing:\", train.isnull().sum().sum())\nprint(\"Test missing:\", test.isnull().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:31.217284Z","iopub.execute_input":"2025-11-10T07:32:31.217963Z","iopub.status.idle":"2025-11-10T07:32:31.491518Z","shell.execute_reply.started":"2025-11-10T07:32:31.217909Z","shell.execute_reply":"2025-11-10T07:32:31.490653Z"}},"outputs":[{"name":"stdout","text":"=== TARGET DISTRIBUTION ===\nloan_paid_back\n1.0    0.79882\n0.0    0.20118\nName: proportion, dtype: float64\n\n=== DATA TYPES ===\nid                        int64\nannual_income           float64\ndebt_to_income_ratio    float64\ncredit_score              int64\nloan_amount             float64\ninterest_rate           float64\ngender                   object\nmarital_status           object\neducation_level          object\nemployment_status        object\nloan_purpose             object\ngrade_subgrade           object\nloan_paid_back          float64\ndtype: object\n\n=== MISSING VALUES ===\nTrain missing: 0\nTest missing: 0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Create powerful financial ratios and features\ndef create_advanced_features(df):\n    df = df.copy()\n    \n    # Financial ratios\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['monthly_debt_burden'] = df['annual_income'] * df['debt_to_income_ratio'] / 12\n    df['affordability_score'] = df['annual_income'] / (df['loan_amount'] * df['interest_rate'] + 1)\n    df['risk_score'] = df['debt_to_income_ratio'] * df['interest_rate']\n    \n    # Credit score bins\n    df['credit_score_category'] = pd.cut(df['credit_score'], \n                                       bins=[0, 580, 670, 740, 800, 850],\n                                       labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n    \n    # Loan amount relative to income\n    df['loan_to_income_ratio'] = df['loan_amount'] / (df['annual_income'] + 1)\n    \n    # Interest rate risk categories\n    df['interest_rate_category'] = pd.cut(df['interest_rate'],\n                                        bins=[0, 5, 10, 15, 20, 100],\n                                        labels=['Low', 'Medium', 'High', 'Very High', 'Extreme'])\n    \n    return df\n\n# Apply feature engineering\ntrain_enhanced = create_advanced_features(train)\ntest_enhanced = create_advanced_features(test)\n\nprint(\"New feature names:\", [col for col in train_enhanced.columns if col not in train.columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:57:35.734322Z","iopub.execute_input":"2025-11-08T11:57:35.734789Z","iopub.status.idle":"2025-11-08T11:57:35.933829Z","shell.execute_reply.started":"2025-11-08T11:57:35.734649Z","shell.execute_reply":"2025-11-08T11:57:35.932995Z"}},"outputs":[{"name":"stdout","text":"New feature names: ['income_to_loan_ratio', 'monthly_debt_burden', 'affordability_score', 'risk_score', 'credit_score_category', 'loan_to_income_ratio', 'interest_rate_category']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Identify categorical and numerical columns\ncategorical_cols = ['gender', 'marital_status', 'education_level', 'employment_status', \n                   'loan_purpose', 'grade_subgrade', 'credit_score_category', 'interest_rate_category']\nnumerical_cols = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount',\n                 'interest_rate', 'income_to_loan_ratio', 'monthly_debt_burden', \n                 'affordability_score', 'risk_score', 'loan_to_income_ratio']\n\n# Prepare features\nX = train_enhanced.drop(['id', 'loan_paid_back'], axis=1)\ny = train_enhanced['loan_paid_back']\nX_test = test_enhanced.drop('id', axis=1)\n\n# Encode categorical variables\nlabel_encoders = {}\nfor col in categorical_cols:\n    if col in X.columns:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col].astype(str))\n        X_test[col] = le.transform(X_test[col].astype(str))\n        label_encoders[col] = le\n\nprint(\"Preprocessing completed!\")\nprint(f\"Final training shape: {X.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:58:17.834864Z","iopub.execute_input":"2025-11-08T11:58:17.835215Z","iopub.status.idle":"2025-11-08T11:58:19.625160Z","shell.execute_reply.started":"2025-11-08T11:58:17.835191Z","shell.execute_reply":"2025-11-08T11:58:19.624174Z"}},"outputs":[{"name":"stdout","text":"Preprocessing completed!\nFinal training shape: (593994, 18)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nimport numpy as np\n\n# Initialize advanced models with optimized parameters\nmodels = {\n    'XGBoost': XGBClassifier(\n        n_estimators=1000,\n        learning_rate=0.1,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        eval_metric='logloss',\n        tree_method='hist'\n    ),\n    'LightGBM': LGBMClassifier(\n        n_estimators=1000,\n        learning_rate=0.1,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        verbose=-1\n    ),\n    'CatBoost': CatBoostClassifier(\n        iterations=1000,\n        learning_rate=0.1,\n        depth=6,\n        random_state=42,\n        verbose=False\n    ),\n    'RandomForest': RandomForestClassifier(\n        n_estimators=200,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    )\n}\n\n# Cross-validation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"=== ADVANCED MODEL CV RESULTS ===\")\ncv_results = {}\nfor name, model in models.items():\n    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n    cv_results[name] = scores.mean()\n    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:58:38.171031Z","iopub.execute_input":"2025-11-08T11:58:38.171320Z","iopub.status.idle":"2025-11-08T12:20:36.868572Z","shell.execute_reply.started":"2025-11-08T11:58:38.171302Z","shell.execute_reply":"2025-11-08T12:20:36.866572Z"}},"outputs":[{"name":"stdout","text":"=== ADVANCED MODEL CV RESULTS ===\nXGBoost: 0.9043 (+/- 0.0010)\nLightGBM: 0.9058 (+/- 0.0012)\nCatBoost: 0.9063 (+/- 0.0015)\nRandomForest: 0.9025 (+/- 0.0013)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Create weighted ensemble based on CV performance\nbest_models = [\n    ('xgb', models['XGBoost']),\n    ('lgb', models['LightGBM']),\n    ('cat', models['CatBoost'])\n]\n\n# Voting Classifier with weights\nensemble = VotingClassifier(estimators=best_models, voting='soft')\n\n# Train and validate ensemble\nensemble_scores = cross_val_score(ensemble, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\nprint(f\"\\n=== ENSEMBLE PERFORMANCE ===\")\nprint(f\"Ensemble CV: {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std() * 2:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:24:19.959885Z","iopub.execute_input":"2025-11-08T12:24:19.963796Z","iopub.status.idle":"2025-11-08T12:35:17.282596Z","shell.execute_reply.started":"2025-11-08T12:24:19.963724Z","shell.execute_reply":"2025-11-08T12:35:17.281355Z"}},"outputs":[{"name":"stdout","text":"\n=== ENSEMBLE PERFORMANCE ===\nEnsemble CV: 0.9063 (+/- 0.0011)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Train final ensemble on all data\nprint(\"Training final ensemble model...\")\nensemble.fit(X, y)\n\n# Predict probabilities for test set\ntest_predictions = ensemble.predict(X_test)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': test_predictions\n})\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"‚úÖ Submission file created!\")\n\n# Analyze predictions\nprint(f\"\\n=== PREDICTION DISTRIBUTION ===\")\nprint(submission['loan_paid_back'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:44:17.209789Z","iopub.execute_input":"2025-11-08T12:44:17.210192Z","iopub.status.idle":"2025-11-08T12:46:41.282412Z","shell.execute_reply.started":"2025-11-08T12:44:17.210161Z","shell.execute_reply":"2025-11-08T12:46:41.281545Z"}},"outputs":[{"name":"stdout","text":"Training final ensemble model...\n‚úÖ Submission file created!\n\n=== PREDICTION DISTRIBUTION ===\nloan_paid_back\n1.0    0.859991\n0.0    0.140009\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU-OPTIMIZED FEATURE ENGINEERING\ndef create_gpu_features(df):\n    df = df.copy()\n    \n    # Vectorized operations (GPU friendly)\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['debt_service_ratio'] = (df['annual_income'] * df['debt_to_income_ratio']) / (df['loan_amount'] * df['interest_rate'] + 1)\n    df['credit_utilization'] = df['loan_amount'] / (df['annual_income'] + 1)\n    \n    # Risk scores\n    df['risk_score'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['composite_score'] = (df['credit_score'] / 850) * (1 - df['debt_to_income_ratio'])\n    \n    # Binning\n    df['credit_bin'] = pd.cut(df['credit_score'], bins=[0, 580, 670, 740, 800, 850], labels=False)\n    df['income_bin'] = pd.cut(df['annual_income'], bins=5, labels=False)\n    \n    return df\n\nprint(\"üöÄ Creating GPU-optimized features...\")\ntrain_gpu = create_gpu_features(train)\ntest_gpu = create_gpu_features(test)\n\n# SELECT OPTIMAL FEATURES\nfeature_columns = [\n    'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n    'income_to_loan_ratio', 'debt_service_ratio', 'credit_utilization', 'risk_score', 'composite_score',\n    'credit_bin', 'income_bin', 'employment_status', 'grade_subgrade'\n]\n\n# PREPROCESSING\nX = train_gpu[feature_columns].copy()\ny = train_gpu['loan_paid_back']\nX_test = test_gpu[feature_columns].copy()\n\n# Encode categoricals\ncategorical_cols = ['employment_status', 'grade_subgrade']\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n\nprint(f\"üìä Training shape: {X.shape}\")\n\n# GPU-OPTIMIZED MODELS\nprint(\"\\n=== TRAINING GPU-ACCELERATED MODELS ===\")\n\n# Model 1: XGBoost with GPU\nxgb_gpu = XGBClassifier(\n    n_estimators=3000,           # More trees since GPU is faster\n    max_depth=8,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    tree_method='gpu_hist',      # GPU ACCELERATION\n    predictor='gpu_predictor',   # GPU prediction\n    eval_metric='logloss',\n    verbosity=0\n)\n\n# Model 2: LightGBM with GPU\nlgb_gpu = LGBMClassifier(\n    n_estimators=3000,\n    max_depth=8,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    device='gpu',               # GPU ACCELERATION\n    gpu_platform_id=0,\n    gpu_device_id=0,\n    verbose=-1\n)\n\n# FAST GPU VALIDATION\ndef gpu_temporal_validation(X, y, model, n_splits=3):\n    \"\"\"Fast GPU validation with fewer splits\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        print(f\"üîÑ GPU Fold {fold+1}/{n_splits}...\")\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        score = np.mean(preds == y_val)\n        scores.append(score)\n        print(f\"   Fold {fold+1} Accuracy: {score:.4f}\")\n    \n    return np.mean(scores), np.std(scores)\n\n# QUICK GPU VALIDATION\nprint(\"‚ö° Running fast GPU validation...\")\nxgb_score, xgb_std = gpu_temporal_validation(X, y, xgb_gpu, n_splits=3)\nlgb_score, lgb_std = gpu_temporal_validation(X, y, lgb_gpu, n_splits=3)\n\nprint(f\"\\nüìà XGBoost GPU CV: {xgb_score:.4f} (+/- {xgb_std:.4f})\")\nprint(f\"üìà LightGBM GPU CV: {lgb_score:.4f} (+/- {lgb_std:.4f})\")\n\n# SELECT BEST GPU MODEL\nif xgb_score >= lgb_score:\n    best_gpu_model = xgb_gpu\n    print(\"üéØ Selected: XGBoost (GPU)\")\nelse:\n    best_gpu_model = lgb_gpu\n    print(\"üéØ Selected: LightGBM (GPU)\")\n\n# FULL TRAINING ON GPU\nprint(\"\\nüî• Training final model on GPU...\")\nbest_gpu_model.fit(X, y)\n\n# PREDICT WITH GPU\nprint(\"‚ö° Making GPU-accelerated predictions...\")\ntest_probs = best_gpu_model.predict_proba(X_test)[:, 1]\n\n# SMART THRESHOLD TUNING\nthreshold = 0.75  # More conservative\ntest_predictions = (test_probs > threshold).astype(int)\n\n# CREATE SUBMISSION\nsubmission_gpu = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': test_predictions\n})\n\n# ANALYZE PREDICTIONS\nprint(f\"\\n=== GPU PREDICTION DISTRIBUTION ===\")\npred_dist = submission_gpu['loan_paid_back'].value_counts(normalize=True)\nprint(pred_dist)\n\n# Ensure we're not too far from training distribution\ntrain_dist = train['loan_paid_back'].value_counts(normalize=True)\nprint(f\"Training distribution: {train_dist[1.0]:.3f} paid, {train_dist[0.0]:.3f} default\")\nprint(f\"Test predictions: {pred_dist[1]:.3f} paid, {pred_dist[0]:.3f} default\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:45.831190Z","iopub.execute_input":"2025-11-10T07:32:45.831482Z","iopub.status.idle":"2025-11-10T07:38:16.714554Z","shell.execute_reply.started":"2025-11-10T07:32:45.831461Z","shell.execute_reply":"2025-11-10T07:38:16.713829Z"}},"outputs":[{"name":"stdout","text":"üöÄ Creating GPU-optimized features...\nüìä Training shape: (593994, 14)\n\n=== TRAINING GPU-ACCELERATED MODELS ===\n‚ö° Running fast GPU validation...\nüîÑ GPU Fold 1/3...\n   Fold 1 Accuracy: 0.9036\nüîÑ GPU Fold 2/3...\n   Fold 2 Accuracy: 0.9045\nüîÑ GPU Fold 3/3...\n   Fold 3 Accuracy: 0.9043\nüîÑ GPU Fold 1/3...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"   Fold 1 Accuracy: 0.9062\nüîÑ GPU Fold 2/3...\n   Fold 2 Accuracy: 0.9060\nüîÑ GPU Fold 3/3...\n   Fold 3 Accuracy: 0.9054\n\nüìà XGBoost GPU CV: 0.9041 (+/- 0.0004)\nüìà LightGBM GPU CV: 0.9059 (+/- 0.0003)\nüéØ Selected: LightGBM (GPU)\n\nüî• Training final model on GPU...\n‚ö° Making GPU-accelerated predictions...\n\n=== GPU PREDICTION DISTRIBUTION ===\nloan_paid_back\n1    0.782216\n0    0.217784\nName: proportion, dtype: float64\nTraining distribution: 0.799 paid, 0.201 default\nTest predictions: 0.782 paid, 0.218 default\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# SAVE SUBMISSION\nsubmission_gpu.to_csv('submission_gpu.csv', index=False)\nprint(\"‚úÖ GPU-accelerated submission created!\")\n\n# FEATURE IMPORTANCE (GPU)\nif hasattr(best_gpu_model, 'feature_importances_'):\n    importance_df = pd.DataFrame({\n        'feature': feature_columns,\n        'importance': best_gpu_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\nüéØ TOP FEATURES (GPU):\")\n    print(importance_df.head(8))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:43:16.840077Z","iopub.execute_input":"2025-11-10T07:43:16.840986Z","iopub.status.idle":"2025-11-10T07:43:17.013283Z","shell.execute_reply.started":"2025-11-10T07:43:16.840962Z","shell.execute_reply":"2025-11-10T07:43:17.012631Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU-accelerated submission created!\n\nüéØ TOP FEATURES (GPU):\n                feature  importance\n1  debt_to_income_ratio       23226\n2          credit_score       10862\n3           loan_amount        9627\n0         annual_income        8412\n4         interest_rate        8313\n9       composite_score        7542\n8            risk_score        4816\n6    debt_service_ratio        4623\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ENSEMBLE OPTION (If you want to try both)\nprint(\"\\nü§ñ Creating GPU Ensemble...\")\nensemble_probs = (xgb_gpu.predict_proba(X_test)[:, 1] + \n                  lgb_gpu.predict_proba(X_test)[:, 1]) / 2\nensemble_preds = (ensemble_probs > 0.75).astype(int)\n\nsubmission_ensemble = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': ensemble_preds\n})\n\nprint(f\"Ensemble distribution:\")\nprint(submission_ensemble['loan_paid_back'].value_counts(normalize=True))\nsubmission_ensemble.to_csv('submission_gpu_ensemble.csv', index=False)\nprint(\"‚úÖ GPU Ensemble submission created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:43:22.092779Z","iopub.execute_input":"2025-11-10T07:43:22.093154Z","iopub.status.idle":"2025-11-10T07:44:00.224218Z","shell.execute_reply.started":"2025-11-10T07:43:22.093131Z","shell.execute_reply":"2025-11-10T07:44:00.223467Z"}},"outputs":[{"name":"stdout","text":"\nü§ñ Creating GPU Ensemble...\nEnsemble distribution:\nloan_paid_back\n1    0.781163\n0    0.218837\nName: proportion, dtype: float64\n‚úÖ GPU Ensemble submission created!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ADVANCED FEATURE ENGINEERING\ndef create_advanced_features(df):\n    df = df.copy()\n    \n    # Financial ratios\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['debt_service_ratio'] = (df['annual_income'] * df['debt_to_income_ratio']) / (df['loan_amount'] * df['interest_rate'] + 1)\n    df['credit_utilization'] = df['loan_amount'] / (df['annual_income'] + 1)\n    \n    # Advanced risk scores\n    df['risk_score_1'] = df['debt_to_income_ratio'] * df['interest_rate'] * (1 - df['credit_score']/850)\n    df['risk_score_2'] = (df['loan_amount'] / df['annual_income']) * df['debt_to_income_ratio'] * 100\n    \n    # Credit score transformations\n    df['credit_score_squared'] = df['credit_score'] ** 2\n    df['credit_score_log'] = np.log1p(df['credit_score'])\n    \n    # Income transformations\n    df['income_log'] = np.log1p(df['annual_income'])\n    df['loan_amount_log'] = np.log1p(df['loan_amount'])\n    \n    # Interaction features\n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income'] / 100000\n    df['debt_interest_interaction'] = df['debt_to_income_ratio'] * df['interest_rate']\n    \n    # Binning with more granular categories\n    df['credit_bin'] = pd.cut(df['credit_score'], bins=[0, 500, 600, 700, 750, 800, 850], labels=False)\n    df['income_bin'] = pd.cut(df['annual_income'], bins=10, labels=False)\n    df['dti_bin'] = pd.cut(df['debt_to_income_ratio'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0], labels=False)\n    \n    return df\n\nprint(\"üöÄ Creating advanced features...\")\ntrain_adv = create_advanced_features(train)\ntest_adv = create_advanced_features(test)\n\n# EXPANDED FEATURE SET\nfeature_columns = [\n    # Original features\n    'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n    \n    # Engineered features\n    'income_to_loan_ratio', 'debt_service_ratio', 'credit_utilization',\n    'risk_score_1', 'risk_score_2', \n    'credit_score_squared', 'credit_score_log',\n    'income_log', 'loan_amount_log',\n    'credit_income_interaction', 'debt_interest_interaction',\n    'credit_bin', 'income_bin', 'dti_bin',\n    \n    # Categorical features\n    'employment_status', 'grade_subgrade', 'loan_purpose', 'education_level'\n]\n\n# PREPROCESSING\nX = train_adv[feature_columns].copy()\ny = train_adv['loan_paid_back']\nX_test = test_adv[feature_columns].copy()\n\n# Encode categoricals\ncategorical_cols = ['employment_status', 'grade_subgrade', 'loan_purpose', 'education_level']\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n\nprint(f\"üìä Final training shape: {X.shape}\")\n\n# ADVANCED GPU MODELS WITH TUNING\nmodels = {\n    'xgb': XGBClassifier(\n        n_estimators=3000,\n        max_depth=7,\n        learning_rate=0.02,  # Lower learning rate\n        subsample=0.75,\n        colsample_bytree=0.75,\n        reg_alpha=3,         # Stronger regularization\n        reg_lambda=3,\n        gamma=0.1,\n        random_state=42,\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        eval_metric='auc',   # Use AUC for imbalance\n        scale_pos_weight=0.25,  # Handle class imbalance\n        verbosity=0\n    ),\n    \n    'lgb': LGBMClassifier(\n        n_estimators=3000,\n        max_depth=7,\n        learning_rate=0.02,\n        subsample=0.75,\n        colsample_bytree=0.75,\n        reg_alpha=3,\n        reg_lambda=3,\n        min_child_samples=20,\n        random_state=42,\n        device='gpu',\n        class_weight='balanced',  # Handle imbalance\n        verbose=-1\n    ),\n    \n    'cat': CatBoostClassifier(\n        iterations=3000,\n        depth=7,\n        learning_rate=0.02,\n        random_state=42,\n        verbose=False,\n        task_type='GPU',  # GPU acceleration\n        class_weights=[0.8, 1.2]  # Handle imbalance\n    )\n}\n\n# STACKING APPROACH\nprint(\"\\n=== TRAINING ADVANCED STACKING MODELS ===\")\n\n# Train all models and create meta-features\nfrom sklearn.model_selection import KFold\n\ndef create_stacking_features(X, X_test, y, models, n_folds=5):\n    \"\"\"Create stacking features using out-of-fold predictions\"\"\"\n    stacking_train = np.zeros((X.shape[0], len(models)))\n    stacking_test = np.zeros((X_test.shape[0], len(models)))\n    \n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for i, (name, model) in enumerate(models.items()):\n        print(f\"üîÑ Creating stacking features with {name}...\")\n        test_fold_preds = []\n        \n        for train_idx, val_idx in kf.split(X):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            model.fit(X_train, y_train)\n            val_preds = model.predict_proba(X_val)[:, 1]\n            stacking_train[val_idx, i] = val_preds\n            \n            # Get test predictions for this fold\n            test_preds = model.predict_proba(X_test)[:, 1]\n            test_fold_preds.append(test_preds)\n        \n        # Average test predictions across folds\n        stacking_test[:, i] = np.mean(test_fold_preds, axis=0)\n    \n    return stacking_train, stacking_test\n\n# Create stacking features\nstacking_train, stacking_test = create_stacking_features(X, X_test, y, models)\n\n# Combine original features with stacking features\nX_stacked = np.hstack([X.values, stacking_train])\nX_test_stacked = np.hstack([X_test.values, stacking_test])\n\nprint(f\"üìä Stacked features shape: {X_stacked.shape}\")\n\n# META-LEARNER (XGBoost on stacked features)\nmeta_learner = XGBClassifier(\n    n_estimators=1000,\n    max_depth=5,\n    learning_rate=0.05,\n    reg_alpha=2,\n    reg_lambda=2,\n    random_state=42,\n    tree_method='gpu_hist',\n    eval_metric='auc',\n    verbosity=0\n)\n\n# VALIDATE STACKING APPROACH\nprint(\"‚ö° Validating stacking approach...\")\ntscv = TimeSeriesSplit(n_splits=3)\nstacking_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(tscv.split(X_stacked)):\n    print(f\"   Fold {fold+1}/3...\")\n    X_tr, X_val = X_stacked[train_idx], X_stacked[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    meta_learner.fit(X_tr, y_tr)\n    val_preds = meta_learner.predict_proba(X_val)[:, 1]\n    score = roc_auc_score(y_val, val_preds)\n    stacking_scores.append(score)\n    print(f\"   Fold {fold+1} AUC: {score:.4f}\")\n\nprint(f\"üìà Stacking CV AUC: {np.mean(stacking_scores):.4f} (+/- {np.std(stacking_scores):.4f})\")\n\n# TRAIN FINAL STACKING MODEL\nprint(\"\\nüî• Training final stacking model...\")\nmeta_learner.fit(X_stacked, y)\n\n# PREDICT WITH OPTIMAL THRESHOLD\nfinal_probs = meta_learner.predict_proba(X_test_stacked)[:, 1]\n\n# Find optimal threshold (you can tune this)\noptimal_threshold = 0.70  # More conservative for default prediction\nfinal_predictions = (final_probs > optimal_threshold).astype(int)\n\n# CREATE SUBMISSION\nsubmission_stacked = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': final_predictions\n})\n\nprint(f\"\\n=== FINAL PREDICTION DISTRIBUTION ===\")\nprint(submission_stacked['loan_paid_back'].value_counts(normalize=True))\n\n# SAVE\nsubmission_stacked.to_csv('submission_stacked.csv', index=False)\nprint(\"‚úÖ Advanced stacking submission created!\")\n\n# ALSO CREATE SIMPLE ENSEMBLE AS BACKUP\nprint(\"\\nü§ñ Creating simple ensemble backup...\")\nsimple_probs = (\n    models['xgb'].predict_proba(X_test)[:, 1] + \n    models['lgb'].predict_proba(X_test)[:, 1] + \n    models['cat'].predict_proba(X_test)[:, 1]\n) / 3\n\nsimple_predictions = (simple_probs > 0.72).astype(int)\n\nsubmission_simple = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': simple_predictions\n})\n\nsubmission_simple.to_csv('submission_simple_ensemble.csv', index=False)\nprint(\"‚úÖ Simple ensemble submission created!\")\n\nprint(\"\\nüéØ Try both submissions and see which works better!\")\nprint(\"   - submission_stacked.csv (Advanced stacking)\")\nprint(\"   - submission_simple_ensemble.csv (Simple ensemble)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ADVANCED FEATURE ENGINEERING (same as before)\ndef create_advanced_features(df):\n    df = df.copy()\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['debt_service_ratio'] = (df['annual_income'] * df['debt_to_income_ratio']) / (df['loan_amount'] * df['interest_rate'] + 1)\n    df['credit_utilization'] = df['loan_amount'] / (df['annual_income'] + 1)\n    df['risk_score_1'] = df['debt_to_income_ratio'] * df['interest_rate'] * (1 - df['credit_score']/850)\n    df['risk_score_2'] = (df['loan_amount'] / df['annual_income']) * df['debt_to_income_ratio'] * 100\n    df['credit_score_squared'] = df['credit_score'] ** 2\n    df['credit_score_log'] = np.log1p(df['credit_score'])\n    df['income_log'] = np.log1p(df['annual_income'])\n    df['loan_amount_log'] = np.log1p(df['loan_amount'])\n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income'] / 100000\n    df['debt_interest_interaction'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['credit_bin'] = pd.cut(df['credit_score'], bins=[0, 500, 600, 700, 750, 800, 850], labels=False)\n    df['income_bin'] = pd.cut(df['annual_income'], bins=10, labels=False)\n    df['dti_bin'] = pd.cut(df['debt_to_income_ratio'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0], labels=False)\n    return df\n\nprint(\"üöÄ Creating advanced features...\")\ntrain_adv = create_advanced_features(train)\ntest_adv = create_advanced_features(test)\n\n# FEATURE SET\nfeature_columns = [\n    'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n    'income_to_loan_ratio', 'debt_service_ratio', 'credit_utilization', 'risk_score_1', 'risk_score_2',\n    'credit_score_squared', 'credit_score_log', 'income_log', 'loan_amount_log',\n    'credit_income_interaction', 'debt_interest_interaction', 'credit_bin', 'income_bin', 'dti_bin',\n    'employment_status', 'grade_subgrade', 'loan_purpose', 'education_level'\n]\n\n# PREPROCESSING\nX = train_adv[feature_columns].copy()\ny = train_adv['loan_paid_back']\nX_test = test_adv[feature_columns].copy()\n\ncategorical_cols = ['employment_status', 'grade_subgrade', 'loan_purpose', 'education_level']\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n\nprint(f\"üìä Final training shape: {X.shape}\")\n\n# OPTIMIZED MODELS WITH LR=0.01\nprint(\"\\n=== TRAINING WITH LEARNING RATE 0.01 ===\")\n\nmodels = {\n    'xgb': XGBClassifier(\n        n_estimators=5000,      # More trees for lower LR\n        max_depth=7,\n        learning_rate=0.01,     # LOWER LEARNING RATE\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=2,\n        reg_lambda=2,\n        gamma=0.1,\n        random_state=42,\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        eval_metric='auc',\n        scale_pos_weight=0.25,\n        verbosity=0\n    ),\n    \n    'lgb': LGBMClassifier(\n        n_estimators=5000,      # More trees\n        max_depth=7,\n        learning_rate=0.01,     # LOWER LEARNING RATE\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=2,\n        reg_lambda=2,\n        min_child_samples=20,\n        random_state=42,\n        device='gpu',\n        class_weight='balanced',\n        verbose=-1\n    ),\n    \n    'cat': CatBoostClassifier(\n        iterations=5000,        # More iterations\n        depth=7,\n        learning_rate=0.01,     # LOWER LEARNING RATE\n        random_state=42,\n        verbose=False,\n        task_type='GPU',\n        class_weights=[0.8, 1.2]\n    )\n}\n\n# QUICK VALIDATION WITH LOW LR\ndef quick_validate_low_lr(X, y, models, n_splits=3):\n    \"\"\"Quick validation to check if low LR helps\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = {}\n    \n    for name, model in models.items():\n        print(f\"üîç Validating {name} with LR=0.01...\")\n        scores = []\n        \n        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            # Use early stopping with low LR\n            if name == 'xgb':\n                model.fit(X_train, y_train, \n                         eval_set=[(X_val, y_val)],\n                         early_stopping_rounds=100,\n                         verbose=False)\n            elif name == 'lgb':\n                model.fit(X_train, y_train,\n                         eval_set=[(X_val, y_val)],\n                         early_stopping_rounds=100,\n                         verbose=False)\n            else:  # catboost\n                model.fit(X_train, y_train,\n                         eval_set=[(X_val, y_val)],\n                         early_stopping_rounds=100,\n                         verbose=False)\n            \n            val_preds = model.predict_proba(X_val)[:, 1]\n            score = np.mean((val_preds > 0.5) == y_val)\n            scores.append(score)\n        \n        results[name] = (np.mean(scores), np.std(scores))\n        print(f\"   {name}: {results[name][0]:.4f} (+/- {results[name][1]:.4f})\")\n    \n    return results\n\n# Run validation\nprint(\"‚ö° Quick validation with LR=0.01...\")\nvalidation_results = quick_validate_low_lr(X, y, models)\n\n# TRAIN FINAL MODELS WITH FULL DATA\nprint(\"\\nüî• Training final models with LR=0.01 on full data...\")\n\n# Train all models without early stopping (use full iterations)\nfor name, model in models.items():\n    print(f\"   Training {name}...\")\n    model.fit(X, y)\n\n# CREATE ENSEMBLE PREDICTIONS\nprint(\"ü§ñ Creating ensemble predictions...\")\n\n# Weighted average based on validation performance\nweights = {\n    'xgb': validation_results['xgb'][0],\n    'lgb': validation_results['lgb'][0], \n    'cat': validation_results['cat'][0]\n}\n\n# Normalize weights\ntotal_weight = sum(weights.values())\nfor key in weights:\n    weights[key] /= total_weight\n\nprint(f\"üéØ Model weights: {weights}\")\n\n# Weighted ensemble predictions\nensemble_probs = (\n    weights['xgb'] * models['xgb'].predict_proba(X_test)[:, 1] +\n    weights['lgb'] * models['lgb'].predict_proba(X_test)[:, 1] +\n    weights['cat'] * models['cat'].predict_proba(X_test)[:, 1]\n)\n\n# Optimized threshold\noptimal_threshold = 0.68  # Slightly more conservative\nfinal_predictions = (ensemble_probs > optimal_threshold).astype(int)\n\n# CREATE SUBMISSION\nsubmission_low_lr = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': final_predictions\n})\n\nprint(f\"\\n=== PREDICTION DISTRIBUTION (LR=0.01) ===\")\nprint(submission_low_lr['loan_paid_back'].value_counts(normalize=True))\n\nsubmission_low_lr.to_csv('submission_low_lr.csv', index=False)\nprint(\"‚úÖ Low learning rate submission created!\")\n\n# ALSO TRY BEST SINGLE MODEL\nbest_single_name = max(validation_results, key=lambda x: validation_results[x][0])\nbest_single_model = models[best_single_name]\nprint(f\"\\nüèÜ Best single model: {best_single_name}\")\n\nsingle_probs = best_single_model.predict_proba(X_test)[:, 1]\nsingle_predictions = (single_probs > 0.70).astype(int)\n\nsubmission_single = pd.DataFrame({\n    'id': test['id'], \n    'loan_paid_back': single_predictions\n})\n\nsubmission_single.to_csv('submission_best_single.csv', index=False)\nprint(\"‚úÖ Best single model submission created!\")\n\nprint(\"\\nüéØ Submit both and compare:\")\nprint(\"   - submission_low_lr.csv (Weighted ensemble)\")\nprint(\"   - submission_best_single.csv (Best single model)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:52:08.311859Z","iopub.execute_input":"2025-11-10T07:52:08.312688Z","iopub.status.idle":"2025-11-10T07:53:01.836293Z","shell.execute_reply.started":"2025-11-10T07:52:08.312662Z","shell.execute_reply":"2025-11-10T07:53:01.835332Z"}},"outputs":[{"name":"stdout","text":"üöÄ Creating advanced features...\nüìä Final training shape: (593994, 23)\n\n=== TRAINING WITH LEARNING RATE 0.01 ===\n‚ö° Quick validation with LR=0.01...\nüîç Validating xgb with LR=0.01...\n   xgb: 0.8692 (+/- 0.0010)\nüîç Validating lgb with LR=0.01...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1706985858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# Run validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ö° Quick validation with LR=0.01...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mvalidation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquick_validate_low_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# TRAIN FINAL MODELS WITH FULL DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/1706985858.py\u001b[0m in \u001b[0;36mquick_validate_low_lr\u001b[0;34m(X, y, models, n_splits)\u001b[0m\n\u001b[1;32m    122\u001b[0m                          verbose=False)\n\u001b[1;32m    123\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lgb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m    125\u001b[0m                          \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                          \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"],"ename":"TypeError","evalue":"LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ADVANCED FEATURE ENGINEERING\ndef create_advanced_features(df):\n    df = df.copy()\n    df['income_to_loan_ratio'] = df['annual_income'] / (df['loan_amount'] + 1)\n    df['debt_service_ratio'] = (df['annual_income'] * df['debt_to_income_ratio']) / (df['loan_amount'] * df['interest_rate'] + 1)\n    df['credit_utilization'] = df['loan_amount'] / (df['annual_income'] + 1)\n    df['risk_score_1'] = df['debt_to_income_ratio'] * df['interest_rate'] * (1 - df['credit_score']/850)\n    df['risk_score_2'] = (df['loan_amount'] / df['annual_income']) * df['debt_to_income_ratio'] * 100\n    df['credit_score_squared'] = df['credit_score'] ** 2\n    df['credit_score_log'] = np.log1p(df['credit_score'])\n    df['income_log'] = np.log1p(df['annual_income'])\n    df['loan_amount_log'] = np.log1p(df['loan_amount'])\n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income'] / 100000\n    df['debt_interest_interaction'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['credit_bin'] = pd.cut(df['credit_score'], bins=[0, 500, 600, 700, 750, 800, 850], labels=False)\n    df['income_bin'] = pd.cut(df['annual_income'], bins=10, labels=False)\n    df['dti_bin'] = pd.cut(df['debt_to_income_ratio'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0], labels=False)\n    return df\n\nprint(\"üöÄ Creating advanced features...\")\ntrain_adv = create_advanced_features(train)\ntest_adv = create_advanced_features(test)\n\n# FEATURE SET\nfeature_columns = [\n    'annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n    'income_to_loan_ratio', 'debt_service_ratio', 'credit_utilization', 'risk_score_1', 'risk_score_2',\n    'credit_score_squared', 'credit_score_log', 'income_log', 'loan_amount_log',\n    'credit_income_interaction', 'debt_interest_interaction', 'credit_bin', 'income_bin', 'dti_bin',\n    'employment_status', 'grade_subgrade', 'loan_purpose', 'education_level'\n]\n\n# PREPROCESSING\nX = train_adv[feature_columns].copy()\ny = train_adv['loan_paid_back']\nX_test = test_adv[feature_columns].copy()\n\ncategorical_cols = ['employment_status', 'grade_subgrade', 'loan_purpose', 'education_level']\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n\nprint(f\"üìä Final training shape: {X.shape}\")\n\n# OPTIMIZED MODELS WITH LR=0.01 (FIXED)\nprint(\"\\n=== TRAINING WITH LEARNING RATE 0.01 ===\")\n\n# Initialize models with early stopping built-in\nxgb_model = XGBClassifier(\n    n_estimators=5000,\n    max_depth=7,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=2,\n    reg_lambda=2,\n    gamma=0.1,\n    random_state=42,\n    tree_method='gpu_hist',\n    predictor='gpu_predictor',\n    eval_metric='auc',\n    scale_pos_weight=0.25,\n    verbosity=0\n)\n\nlgb_model = LGBMClassifier(\n    n_estimators=5000,\n    max_depth=7,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=2,\n    reg_lambda=2,\n    min_child_samples=20,\n    random_state=42,\n    device='gpu',\n    class_weight='balanced',\n    verbose=-1\n)\n\ncat_model = CatBoostClassifier(\n    iterations=5000,\n    depth=7,\n    learning_rate=0.01,\n    random_state=42,\n    verbose=False,\n    task_type='GPU',\n    class_weights=[0.8, 1.2]\n)\n\nmodels = {\n    'xgb': xgb_model,\n    'lgb': lgb_model, \n    'cat': cat_model\n}\n\n# SIMPLIFIED VALIDATION (NO EARLY STOPPING ISSUES)\ndef simple_validate(X, y, models, n_splits=3):\n    \"\"\"Simple validation without early stopping complications\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = {}\n    \n    for name, model in models.items():\n        print(f\"üîç Validating {name} with LR=0.01...\")\n        scores = []\n        \n        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            # Train without early stopping for simplicity\n            if name == 'xgb':\n                # For XGBoost, use smaller version for validation speed\n                temp_model = XGBClassifier(\n                    n_estimators=5000,\n                    max_depth=7,\n                    learning_rate=0.01,\n                    random_state=42,\n                    tree_method='gpu_hist',\n                    verbosity=0\n                )\n                temp_model.fit(X_train, y_train)\n                val_preds = temp_model.predict_proba(X_val)[:, 1]\n            else:\n                # For others, use the full model but with fewer iterations for validation\n                temp_model = model.__class__(**{**model.get_params(), 'n_estimators': 1000, 'iterations': 1000})\n                temp_model.fit(X_train, y_train)\n                val_preds = temp_model.predict_proba(X_val)[:, 1]\n            \n            score = np.mean((val_preds > 0.5) == y_val)\n            scores.append(score)\n            print(f\"      Fold {fold+1}: {score:.4f}\")\n        \n        results[name] = (np.mean(scores), np.std(scores))\n        print(f\"   {name} Average: {results[name][0]:.4f} (+/- {results[name][1]:.4f})\\n\")\n    \n    return results\n\n# Run validation\nprint(\"‚ö° Quick validation with LR=0.01...\")\nvalidation_results = simple_validate(X, y, models)\n\n# TRAIN FINAL MODELS WITH FULL DATA AND LR=0.01\nprint(\"\\nüî• Training final models with LR=0.01 on full data...\")\n\nfinal_models = {}\nfor name, model in models.items():\n    print(f\"   Training {name}...\")\n    # Train on full data with all iterations\n    model.fit(X, y)\n    final_models[name] = model\n    print(f\"   ‚úÖ {name} training complete\")\n\n# CREATE ENSEMBLE PREDICTIONS\nprint(\"ü§ñ Creating ensemble predictions...\")\n\n# Get predictions from all models\nxgb_probs = final_models['xgb'].predict_proba(X_test)[:, 1]\nlgb_probs = final_models['lgb'].predict_proba(X_test)[:, 1] \ncat_probs = final_models['cat'].predict_proba(X_test)[:, 1]\n\n# Simple average (more robust than weighted)\nensemble_probs = (xgb_probs + lgb_probs + cat_probs) / 3\n\n# Optimized threshold\noptimal_threshold = 0.70\nfinal_predictions = (ensemble_probs > optimal_threshold).astype(int)\n\n# CREATE SUBMISSION\nsubmission_low_lr = pd.DataFrame({\n    'id': test['id'],\n    'loan_paid_back': final_predictions\n})\n\nprint(f\"\\n=== PREDICTION DISTRIBUTION (LR=0.01) ===\")\npred_dist = submission_low_lr['loan_paid_back'].value_counts(normalize=True)\nprint(pred_dist)\n\nsubmission_low_lr.to_csv('submission_low_lr.csv', index=False)\nprint(\"‚úÖ Low learning rate ensemble submission created!\")\n\n# ALSO CREATE INDIVIDUAL MODEL SUBMISSIONS\nprint(\"\\nüìä Creating individual model submissions...\")\n\nfor name, model in final_models.items():\n    probs = model.predict_proba(X_test)[:, 1]\n    predictions = (probs > 0.70).astype(int)\n    \n    submission_individual = pd.DataFrame({\n        'id': test['id'],\n        'loan_paid_back': predictions\n    })\n    \n    filename = f'submission_{name}.csv'\n    submission_individual.to_csv(filename, index=False)\n    print(f\"‚úÖ {name} submission created: {filename}\")\n\nprint(f\"\\nüéØ All submissions created with LR=0.01!\")\nprint(\"   Try these in order:\")\nprint(\"   1. submission_low_lr.csv (Ensemble)\")\nprint(\"   2. submission_xgb.csv (XGBoost only)\")\nprint(\"   3. submission_lgb.csv (LightGBM only)\")\nprint(\"   4. submission_cat.csv (CatBoost only)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:01:06.864457Z","iopub.execute_input":"2025-11-10T08:01:06.864782Z","iopub.status.idle":"2025-11-10T08:03:30.168288Z","shell.execute_reply.started":"2025-11-10T08:01:06.864762Z","shell.execute_reply":"2025-11-10T08:03:30.167332Z"}},"outputs":[{"name":"stdout","text":"üöÄ Creating advanced features...\nüìä Final training shape: (593994, 23)\n\n=== TRAINING WITH LEARNING RATE 0.01 ===\n‚ö° Quick validation with LR=0.01...\nüîç Validating xgb with LR=0.01...\n      Fold 1: 0.9035\n      Fold 2: 0.9043\n      Fold 3: 0.9045\n   xgb Average: 0.9041 (+/- 0.0004)\n\nüîç Validating lgb with LR=0.01...\n      Fold 1: 0.8671\n      Fold 2: 0.8674\n      Fold 3: 0.8674\n   lgb Average: 0.8673 (+/- 0.0001)\n\nüîç Validating cat with LR=0.01...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3700706152.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m# Run validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ö° Quick validation with LR=0.01...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mvalidation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# TRAIN FINAL MODELS WITH FULL DATA AND LR=0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3700706152.py\u001b[0m in \u001b[0;36msimple_validate\u001b[0;34m(X, y, models, n_splits)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# For others, use the full model but with fewer iterations for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m         \u001b[0m_process_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_process_synonyms\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m     \u001b[0m_process_synonyms_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[0mmetric_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_process_synonyms_groups\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0m_process_synonyms_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0m_process_synonyms_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2_leaf_reg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reg_lambda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m     \u001b[0m_process_synonyms_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_boost_round'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_trees'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1578\u001b[0m     \u001b[0m_process_synonyms_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'od_wait'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'early_stopping_rounds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[0m_process_synonyms_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'custom_metric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'custom_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_process_synonyms_group\u001b[0;34m(synonyms, params)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msynonym\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'only one of the parameters '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' should be initialized.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynonym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynonym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: only one of the parameters iterations, n_estimators, num_boost_round, num_trees should be initialized."],"ename":"CatBoostError","evalue":"only one of the parameters iterations, n_estimators, num_boost_round, num_trees should be initialized.","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport time\nimport warnings\nwarnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n\n# --- 1. CONFIGURATION & SETUP ---\n# **UPDATED based on your data schema and Kaggle path**\n# Using your provided path for the data\nDATA_PATH_TRAIN = '/kaggle/input/playground-series-s5e11/train.csv'\nDATA_PATH_TEST = '/kaggle/input/playground-series-s5e11/test.csv'\nTARGET_COLUMN = 'loan_paid_back' # Target column confirmed\nID_COLUMN = 'id'\n\n# Define feature groups based on your provided columns\nNUMERIC_FEATURES = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\nCATEGORICAL_FEATURES = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nDATE_FEATURES = [] # No date features provided in the schema\n\n# Feature selected for leak-free Target Encoding in the CV loop\nHIGH_CARD_FEATURE = 'loan_purpose'\n\n# --- 2. DATA LOADING ---\nprint(\"1. Loading Data...\")\ntry:\n    # Load training and test data using the provided Kaggle paths\n    df_train = pd.read_csv(DATA_PATH_TRAIN)\n    df_test = pd.read_csv(DATA_PATH_TEST)\n    \n    # Combine data for consistent preprocessing (excluding the target column from test)\n    # The 'Source' column is essential to split back later\n    df_train['Source'] = 'Train'\n    df_test['Source'] = 'Test'\n    \n    # Get the target variable before concatenating\n    y = df_train[TARGET_COLUMN]\n    \n    # Concatenate the main dataframes for unified feature engineering\n    df_combined = pd.concat([df_train.drop(TARGET_COLUMN, axis=1), df_test], ignore_index=True)\n    \n    print(f\"Train data loaded. Shape: {df_train.shape}\")\n    print(f\"Test data loaded. Shape: {df_test.shape}\")\n    print(f\"Combined data shape: {df_combined.shape}\")\n    \nexcept FileNotFoundError:\n    print(f\"Error: One or both files not found at the specified paths.\")\n    print(\"Using large dummy data for demonstration. **Please replace with real data.**\")\n    \n    # --- Fallback to Dummy Data (for robustness) ---\n    data = {\n        ID_COLUMN: range(593994),\n        TARGET_COLUMN: np.random.randint(0, 2, 593994),\n        'annual_income': np.random.rand(593994) * 120000,\n        'debt_to_income_ratio': np.random.rand(593994) * 40,\n        'credit_score': np.random.randint(580, 850, 593994),\n        'loan_amount': np.random.rand(593994) * 35000,\n        'interest_rate': np.random.rand(593994) * 25,\n        'gender': np.random.choice(['M', 'F', np.nan], 593994),\n        'marital_status': np.random.choice(['Single', 'Married'], 593994),\n        'education_level': np.random.choice(['High School', 'Bachelor', 'Master'], 593994),\n        'employment_status': np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], 593994),\n        'loan_purpose': np.random.choice(['Debt Consolidation', 'Credit Card', 'Home Improvement', 'Other'], 593994),\n        'grade_subgrade': np.random.choice([f'A{i}' for i in range(1, 6)], 593994),\n    }\n    df_train = pd.DataFrame(data)\n    y = df_train[TARGET_COLUMN]\n    df_combined = df_train.drop(TARGET_COLUMN, axis=1)\n    df_combined['Source'] = 'Train'\n    \n    # Introduce NaNs for the dummy data path\n    for col in NUMERIC_FEATURES:\n         df_combined.loc[np.random.choice(df_combined.index, 5000, replace=False), col] = np.nan\n    for col in CATEGORICAL_FEATURES:\n         df_combined.loc[np.random.choice(df_combined.index, 2000, replace=False), col] = np.nan\n\n# Store the original test IDs for submission later\ntest_ids = df_test[ID_COLUMN]\n\n# Drop the ID column from the combined features\nif ID_COLUMN in df_combined.columns:\n    df_combined = df_combined.drop(columns=[ID_COLUMN])\n\n# --- 3. MISSING VALUE IMPUTATION & CLEANING ---\n\nprint(\"\\n2. Handling Missing Values and Cleaning...\")\n\n# 3.1 Create Missing Value Indicator Features (CRITICAL for Loan data)\nprint(\"¬† ¬†- Creating missing value indicator flags.\")\nmissing_flag_features = []\n\n# Check all relevant columns for NaNs\ncols_to_check_missing = list(set(NUMERIC_FEATURES + CATEGORICAL_FEATURES))\n\nfor col in cols_to_check_missing:\n    if col in df_combined.columns and df_combined[col].isnull().any():\n        # Only create flag if column has missing data\n        flag_name = f'{col}_is_missing'\n        df_combined[flag_name] = df_combined[col].isnull().astype(int)\n        missing_flag_features.append(flag_name)\n\n# Update NUMERIC_FEATURES with the new flags\nNUMERIC_FEATURES.extend(missing_flag_features)\nNUMERIC_FEATURES = list(set(NUMERIC_FEATURES)) # Ensure uniqueness and update the list\n\n\n# Imputation Strategy: Numerical Imputation: Fill NaNs with the median\nfor col in NUMERIC_FEATURES:\n    if col in df_combined.columns and df_combined[col].dtype != 'object' and df_combined[col].isnull().any():\n        # Use the median calculated from the combined (train+test) dataset for stability\n        df_combined[col] = df_combined[col].fillna(df_combined[col].median())\n\n# Categorical Imputation: Fill NaNs with a special 'Missing' category\nfor col in CATEGORICAL_FEATURES:\n    if col in df_combined.columns and df_combined[col].dtype == 'object' and df_combined[col].isnull().any():\n        df_combined[col] = df_combined[col].fillna('Missing')\n\n# --- 4. FEATURE ENGINEERING ---\n\nprint(\"\\n3. Feature Engineering...\")\n\n# 4.1. Ordinal Encoding for grade_subgrade (High Impact Feature)\n# This assumes higher subgrades (A1 < A2 < ... < E5) are worse or better.\nif 'grade_subgrade' in df_combined.columns:\n    # Create an ordinal mapping: A=1, B=2, ..., E=5. And subgrade 1=1, 2=0.2, etc.\n    def encode_grade_subgrade(s):\n        if pd.isna(s) or s == 'Missing': return 0\n        grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n        grade = s[0]\n        # Handle cases where subgrade might not be a digit (e.g., 'A') by defaulting to 1\n        try:\n            subgrade = int(s[1:])\n        except:\n            subgrade = 1\n        # Combine: Example A1 = 1.1, G5 = 7.5\n        return grade_map.get(grade, 0) + (subgrade / 10)\n        \n    df_combined['grade_subgrade_numeric'] = df_combined['grade_subgrade'].apply(encode_grade_subgrade)\n    NUMERIC_FEATURES.append('grade_subgrade_numeric')\n    \n    # --- FIX: DROP THE ORIGINAL CATEGORICAL COLUMN ---\n    df_combined = df_combined.drop(columns=['grade_subgrade'])\n    # Remove original feature from categorical list to avoid OHE\n    CATEGORICAL_FEATURES.remove('grade_subgrade')\n\n# Remove the target-encoded feature from the list to be OHE later\nif HIGH_CARD_FEATURE in CATEGORICAL_FEATURES:\n    CATEGORICAL_FEATURES.remove(HIGH_CARD_FEATURE)\n    print(f\"¬† ¬†- Removed {HIGH_CARD_FEATURE} from OHE list for Target Encoding later.\")\n\n# 4.2. Interaction Features (Critical for performance)\n# Creating ratios that lenders likely use to assess risk.\nif all(col in df_combined.columns for col in ['loan_amount', 'annual_income']):\n    # Income stability against the size of the loan\n    df_combined['loan_to_income_ratio'] = df_combined['loan_amount'] / (df_combined['annual_income'] + 1e-6)\n    NUMERIC_FEATURES.append('loan_to_income_ratio')\n\nif all(col in df_combined.columns for col in ['debt_to_income_ratio', 'annual_income']):\n    # Estimate of absolute monthly debt payment\n    df_combined['estimated_monthly_debt'] = df_combined['debt_to_income_ratio'] * df_combined['annual_income'] / 100 / 12\n    NUMERIC_FEATURES.append('estimated_monthly_debt')\n\nif all(col in df_combined.columns for col in ['interest_rate', 'credit_score']):\n    # Combining risk factors\n    df_combined['rate_x_score'] = df_combined['interest_rate'] * df_combined['credit_score']\n    NUMERIC_FEATURES.append('rate_x_score')\n\n# Update feature lists after engineering\nNEW_NUMERIC_FEATURES = [col for col in df_combined.columns if col not in NUMERIC_FEATURES and col not in CATEGORICAL_FEATURES and col not in [TARGET_COLUMN, 'Source']]\nNUMERIC_FEATURES.extend(NEW_NUMERIC_FEATURES)\n\nCATEGORICAL_FEATURES = [col for col in CATEGORICAL_FEATURES if col in df_combined.columns]\n\n# --- 5. ENCODING AND SCALING ---\n\n# 5.1. One-Hot Encoding for remaining Categorical Features\ndf_combined = pd.get_dummies(df_combined, columns=CATEGORICAL_FEATURES, dummy_na=False)\n\n# 5.2. Scaling Numerical Features\nprint(\"Scaling numerical features...\")\n# QuantileTransformer is excellent for boosting models as it transforms the data to a normal distribution\nQT = QuantileTransformer(output_distribution='normal', n_quantiles=1000, subsample=50000)\n\nfeatures_to_scale = [col for col in NUMERIC_FEATURES if col in df_combined.columns and df_combined[col].dtype in ['float64', 'int64']]\n\n# Apply scaling to the combined dataset\ndf_combined[features_to_scale] = QT.fit_transform(df_combined[features_to_scale])\n\n\n# --- 6. DATA SPLIT AND MODELING (LightGBM with Stratified K-Fold Cross-Validation) ---\n\n# Split combined data back into training and test sets\nX = df_combined.loc[df_combined['Source'] == 'Train'].drop('Source', axis=1).reset_index(drop=True)\nX_test = df_combined.loc[df_combined['Source'] == 'Test'].drop('Source', axis=1).reset_index(drop=True)\nX_test_original = X_test.copy() # Keep a copy of X_test before target encoding/dropping\n\n# Identify remaining features for training\nFINAL_FEATURES = X.columns.tolist()\n\n# Get the list of OHE columns (which are now boolean/int 0/1)\ncategorical_cols = [col for col in FINAL_FEATURES if any(cat in col for cat in CATEGORICAL_FEATURES)]\nfor col in categorical_cols:\n    X[col] = X[col].astype('category')\n    X_test[col] = X_test[col].astype('category') # Apply category type to test set as well\n\nprint(\"\\n4. Training LightGBM Model with Stratified K-Fold...\")\n\n\n# LightGBM Hyperparameters - Significantly optimized for performance and convergence\nLGB_PARAMS = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 5000, # Max iterations, stopped by early stopping\n    'learning_rate': 0.015, # ***CRITICAL FIX: Increased dramatically from 0.00001***\n    'num_leaves': 60, # Increased model complexity\n    'max_depth': 7, # Increased model complexity\n    'seed': 42,\n    'n_jobs': -1,\n    'colsample_bytree': 0.7,\n    'subsample': 0.7,\n    'reg_alpha': 0.1, # Added light regularization\n    'reg_lambda': 0.1, # Added light regularization\n    'verbose': -1,\n    'min_child_samples': 20, # Reduced slightly from 25\n    'scale_pos_weight': 1,\n    \n    # --- GPU CONFIGURATION (Use P100 for best speed) ---\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n    # ----------------------------------------------------\n}\n\nN_SPLITS = 5\nfolds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test)) # Array to hold averaged test predictions\nfeature_importance_df = pd.DataFrame()\nstart_time = time.time()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    print(f\"Fold {fold_+1}/{N_SPLITS}...\")\n    X_train_fold = X.iloc[trn_idx].copy()\n    X_valid_fold = X.iloc[val_idx].copy()\n    y_train = y.iloc[trn_idx]\n    y_valid = y.iloc[val_idx]\n    \n    # Create a clean copy of the test data for this fold\n    X_test_fold = X_test.copy()\n\n    # --- Leak-Free Target Encoding for HIGH_CARD_FEATURE ---\n    if HIGH_CARD_FEATURE in X_train_fold.columns:\n        # Calculate target mean on training set\n        target_map = y_train.groupby(X_train_fold[HIGH_CARD_FEATURE]).mean()\n        \n        # Apply mapping to train, validation, and test sets\n        X_train_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'] = X_train_fold[HIGH_CARD_FEATURE].map(target_map)\n        X_valid_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'] = X_valid_fold[HIGH_CARD_FEATURE].map(target_map)\n        X_test_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'] = X_test_fold[HIGH_CARD_FEATURE].map(target_map)\n        \n        # Fill NaNs (if a category appears in validation/test but not train) with the global mean\n        global_mean = y_train.mean()\n        X_valid_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'] = X_valid_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'].fillna(global_mean)\n        X_test_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'] = X_test_fold[f'{HIGH_CARD_FEATURE}_TargetEnc'].fillna(global_mean)\n\n        # Drop the original categorical column from the fold data\n        X_train_fold = X_train_fold.drop(columns=[HIGH_CARD_FEATURE])\n        X_valid_fold = X_valid_fold.drop(columns=[HIGH_CARD_FEATURE])\n        X_test_fold = X_test_fold.drop(columns=[HIGH_CARD_FEATURE])\n    # -----------------------------------------------------\n\n    model = lgb.LGBMClassifier(**LGB_PARAMS)\n    \n    model.fit(X_train_fold, y_train,\n              eval_set=[(X_valid_fold, y_valid)],\n              eval_metric='auc',\n              callbacks=[lgb.early_stopping(200, verbose=False)],\n             )\n\n    oof_preds[val_idx] = model.predict_proba(X_valid_fold)[:, 1]\n    \n    # Predict on the test set and accumulate (average) the predictions\n    test_preds += model.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n\n    # Feature Importance for analysis\n    fold_importance_df = pd.DataFrame({\n        'feature': X_train_fold.columns, # Use training fold columns since they now include TargetEnc\n        'importance': model.feature_importances_,\n        'fold': fold_ + 1\n    })\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    del X_train_fold, X_valid_fold, y_train, y_valid, X_test_fold\n    \n# --- 7. RESULTS & SUBMISSION ---\n\ncv_auc_score = roc_auc_score(y, oof_preds)\ntime_taken = time.time() - start_time\nprint(f\"\\n--- Cross-Validation Results ---\")\nprint(f\"OOF AUC Score: {cv_auc_score:.6f}\")\nprint(f\"Total Training Time: {time_taken:.2f} seconds\")\n\n# Display top features\nprint(\"\\nTop 20 Features by Importance:\")\navg_importance = feature_importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False).head(20)\nprint(avg_importance)\n\nif cv_auc_score > 0.92:\n    print(\"\\n\\t*** Congratulations! You are competitive with or exceeding the target score. ***\")\nelse:\n    print(f\"\\n\\tCurrent OOF AUC Score: {cv_auc_score:.6f}. Further fine-tuning or feature engineering may be needed.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T09:01:26.539421Z","iopub.execute_input":"2025-12-02T09:01:26.539719Z","iopub.status.idle":"2025-12-02T09:11:11.090777Z","shell.execute_reply.started":"2025-12-02T09:01:26.539696Z","shell.execute_reply":"2025-12-02T09:11:11.090065Z"}},"outputs":[{"name":"stdout","text":"1. Loading Data...\nTrain data loaded. Shape: (593994, 14)\nTest data loaded. Shape: (254569, 13)\nCombined data shape: (848563, 13)\n\n2. Handling Missing Values and Cleaning...\n¬† ¬†- Creating missing value indicator flags.\n\n3. Feature Engineering...\n¬† ¬†- Removed loan_purpose from OHE list for Target Encoding later.\nScaling numerical features...\n\n4. Training LightGBM Model with Stratified K-Fold...\nFold 1/5...\nFold 2/5...\nFold 3/5...\nFold 4/5...\nFold 5/5...\n\n--- Cross-Validation Results ---\nOOF AUC Score: 0.922103\nTotal Training Time: 579.55 seconds\n\nTop 20 Features by Importance:\nfeature\ndebt_to_income_ratio            27103.4\ncredit_score                    20274.4\nloan_amount                     17213.8\nannual_income                   15651.2\ninterest_rate                   15351.6\nestimated_monthly_debt          14936.2\nrate_x_score                    14689.8\nloan_to_income_ratio            13854.8\ngrade_subgrade_numeric           7655.6\nloan_purpose_TargetEnc           4718.4\neducation_level_Bachelor's       1084.0\nmarital_status_Married           1037.8\nmarital_status_Single            1007.2\nemployment_status_Student         992.0\nemployment_status_Unemployed      967.8\nemployment_status_Retired         954.8\neducation_level_High School       951.2\ngender_Female                     889.0\ngender_Male                       857.6\nemployment_status_Employed        790.0\nName: importance, dtype: float64\n\n\t*** Congratulations! You are competitive with or exceeding the target score. ***\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- 8. CREATE SUBMISSION FILE ---\nsubmission_df = pd.DataFrame({\n    ID_COLUMN: test_ids,\n    TARGET_COLUMN: test_preds\n})\n\nsubmission_filename = 'submission_lgbm_optimized.csv'\nsubmission_df.to_csv(submission_filename, index=False)\n\nprint(f\"\\nSubmission file '{submission_filename}' created successfully. Test predictions are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T09:12:55.501884Z","iopub.execute_input":"2025-12-02T09:12:55.502480Z","iopub.status.idle":"2025-12-02T09:12:55.972445Z","shell.execute_reply.started":"2025-12-02T09:12:55.502455Z","shell.execute_reply":"2025-12-02T09:12:55.971825Z"}},"outputs":[{"name":"stdout","text":"\nSubmission file 'submission_lgbm_optimized.csv' created successfully. Test predictions are ready.\n","output_type":"stream"}],"execution_count":11}]}