{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:03:45.974808Z","iopub.execute_input":"2025-12-04T10:03:45.975060Z","iopub.status.idle":"2025-12-04T10:03:47.864007Z","shell.execute_reply.started":"2025-12-04T10:03:45.975031Z","shell.execute_reply":"2025-12-04T10:03:47.863156Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e12/sample_submission.csv\n/kaggle/input/playground-series-s5e12/train.csv\n/kaggle/input/playground-series-s5e12/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:04:54.682350Z","iopub.execute_input":"2025-12-04T10:04:54.682971Z","iopub.status.idle":"2025-12-04T10:04:59.893750Z","shell.execute_reply.started":"2025-12-04T10:04:54.682947Z","shell.execute_reply":"2025-12-04T10:04:59.893115Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- 1. Load Data ---\n# Define the file paths based on your Kaggle notebook structure\nTRAIN_FILE = \"/kaggle/input/playground-series-s5e12/train.csv\"\nTEST_FILE = \"/kaggle/input/playground-series-s5e12/test.csv\"\nSUB_FILE = \"/kaggle/input/playground-series-s5e12/sample_submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:06:24.421497Z","iopub.execute_input":"2025-12-04T10:06:24.422378Z","iopub.status.idle":"2025-12-04T10:06:24.426019Z","shell.execute_reply.started":"2025-12-04T10:06:24.422353Z","shell.execute_reply":"2025-12-04T10:06:24.425323Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_FILE)\ntest_df = pd.read_csv(TEST_FILE)\nsubmission_df = pd.read_csv(SUB_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:09:25.635686Z","iopub.execute_input":"2025-12-04T10:09:25.636409Z","iopub.status.idle":"2025-12-04T10:09:28.126190Z","shell.execute_reply.started":"2025-12-04T10:09:25.636387Z","shell.execute_reply":"2025-12-04T10:09:28.125584Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:54:10.682811Z","iopub.execute_input":"2025-12-04T11:54:10.683069Z","iopub.status.idle":"2025-12-04T11:54:10.687903Z","shell.execute_reply.started":"2025-12-04T11:54:10.683052Z","shell.execute_reply":"2025-12-04T11:54:10.687180Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'age', 'alcohol_consumption_per_week',\n       'physical_activity_minutes_per_week', 'diet_score',\n       'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n       'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n       'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol',\n       'triglycerides', 'gender', 'ethnicity', 'education_level',\n       'income_level', 'smoking_status', 'employment_status',\n       'family_history_diabetes', 'hypertension_history',\n       'cardiovascular_history', 'diagnosed_diabetes'],\n      dtype='object')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"test_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:54:14.701368Z","iopub.execute_input":"2025-12-04T11:54:14.701635Z","iopub.status.idle":"2025-12-04T11:54:14.706817Z","shell.execute_reply.started":"2025-12-04T11:54:14.701614Z","shell.execute_reply":"2025-12-04T11:54:14.706064Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'age', 'alcohol_consumption_per_week',\n       'physical_activity_minutes_per_week', 'diet_score',\n       'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n       'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n       'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol',\n       'triglycerides', 'gender', 'ethnicity', 'education_level',\n       'income_level', 'smoking_status', 'employment_status',\n       'family_history_diabetes', 'hypertension_history',\n       'cardiovascular_history'],\n      dtype='object')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Separate features (X) and target (y)\nTARGET = 'diagnosed_diabetes'\nX = train_df.drop(['id', TARGET], axis=1)\ny = train_df[TARGET]\nX_test = test_df.drop('id', axis=1)\n\n# Identify categorical columns (usually 'object' dtype in pandas)\ncat_cols = X.select_dtypes(include=['object']).columns\n\n\n# Simple Label Encoding for LightGBM to handle categories natively\n# We must fit the encoder on the combined train/test data to avoid unseen labels in test\nfor col in cat_cols:\n    le = LabelEncoder()\n    # Combine columns to ensure all unique categories are encoded consistently\n    combined = pd.concat([X[col], X_test[col]]).astype(str).fillna('missing')\n    le.fit(combined)\n    X[col] = le.transform(X[col].astype(str).fillna('missing'))\n    X_test[col] = le.transform(X_test[col].astype(str).fillna('missing'))\n    # Set the dtype back to 'category' for LightGBM to utilize its native handling\n    X[col] = X[col].astype('category')\n    X_test[col] = X_test[col].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:56:47.930879Z","iopub.execute_input":"2025-12-04T10:56:47.931618Z","iopub.status.idle":"2025-12-04T10:56:49.799892Z","shell.execute_reply.started":"2025-12-04T10:56:47.931595Z","shell.execute_reply":"2025-12-04T10:56:49.799171Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- 3. Model Configuration (LightGBM with GPU) ---\n\n# LightGBM Parameters - Optimized for Binary Classification and GPU usage\n# These parameters are a good starting point for high AUC.\nlgb_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 6000,           # High number, but Early Stopping controls actual training\n    'learning_rate': 0.01,          # Slower learning rate usually gives better generalization\n    'num_leaves': 31,               # Controls tree complexity\n    'max_depth': -1,                # No limit on tree depth\n    'colsample_bytree': 0.7,        # Feature subsampling\n    'subsample': 0.7,               # Data subsampling (bagging)\n    'random_state': 42,\n    \n    # --- GPU Configuration ---\n    # This enables GPU acceleration. Ensure your Kaggle notebook accelerator is ON (P100 or T4).\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    # -------------------------\n}\n\n# --- 4. Stratified K-Fold Cross-Validation ---\nFOLDS = 5 # A good balance between stability and training time\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n# Arrays to store out-of-fold predictions and final test predictions\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\ncv_scores = []\n\nprint(\"--- Starting LightGBM Cross-Validation Training ---\")\n\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(f\"\\n⚡️ Fold {fold+1}/{FOLDS}\")\n    \n    # Split data for this fold\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    # Initialize the model\n    model = lgb.LGBMClassifier(**lgb_params)\n\n    # Train the model with early stopping\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='auc',\n        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=100)]\n    )\n\n    # Predict on the validation set for OOF prediction\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_index] = val_preds\n    \n    # Calculate and store the AUC for this fold\n    fold_auc = roc_auc_score(y_val, val_preds)\n    cv_scores.append(fold_auc)\n    print(f\"Fold {fold+1} AUC: {fold_auc:.5f}\")\n\n    # Predict on the test set and accumulate (Averaging predictions over all folds)\n    test_preds += model.predict_proba(X_test)[:, 1] / FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:21:42.283405Z","iopub.execute_input":"2025-12-04T11:21:42.284094Z","iopub.status.idle":"2025-12-04T11:47:13.567692Z","shell.execute_reply.started":"2025-12-04T11:21:42.284072Z","shell.execute_reply":"2025-12-04T11:47:13.566846Z"}},"outputs":[{"name":"stdout","text":"--- Starting LightGBM Cross-Validation Training ---\n\n⚡️ Fold 1/5\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1645\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 19 dense feature groups (10.68 MB) transferred to GPU in 0.012027 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[5738]\tvalid_0's auc: 0.728041\nFold 1 AUC: 0.72804\n\n⚡️ Fold 2/5\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1646\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 19 dense feature groups (10.68 MB) transferred to GPU in 0.011807 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[6000]\tvalid_0's auc: 0.725956\nFold 2 AUC: 0.72596\n\n⚡️ Fold 3/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1648\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 19 dense feature groups (10.68 MB) transferred to GPU in 0.011856 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[5989]\tvalid_0's auc: 0.727241\nFold 3 AUC: 0.72724\n\n⚡️ Fold 4/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1645\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 19 dense feature groups (10.68 MB) transferred to GPU in 0.012139 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[5992]\tvalid_0's auc: 0.728001\nFold 4 AUC: 0.72800\n\n⚡️ Fold 5/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1644\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 19 dense feature groups (10.68 MB) transferred to GPU in 0.011701 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[5994]\tvalid_0's auc: 0.728229\nFold 5 AUC: 0.72823\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Calculate the final CV score\nmean_auc = np.mean(cv_scores)\nprint(\"\\n\" + \"=\"*40)\nprint(f\"OVERALL OOF AUC: {mean_auc:.5f} (Target Metric)\")\nprint(\"=\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:49:38.032382Z","iopub.execute_input":"2025-12-04T11:49:38.033155Z","iopub.status.idle":"2025-12-04T11:49:38.037266Z","shell.execute_reply.started":"2025-12-04T11:49:38.033123Z","shell.execute_reply":"2025-12-04T11:49:38.036691Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nOVERALL OOF AUC: 0.72749 (Target Metric)\n========================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Create the submission file\nsubmission_df[TARGET] = test_preds\n\n# Save the submission file\nsubmission_file_path = \"submission.csv\"\nsubmission_df.to_csv(submission_file_path, index=False)\n\nprint(f\"\\n✅ Submission file '{submission_file_path}' created successfully!\")\nprint(f\"   Shape: {submission_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:49:41.226904Z","iopub.execute_input":"2025-12-04T11:49:41.227477Z","iopub.status.idle":"2025-12-04T11:49:41.790368Z","shell.execute_reply.started":"2025-12-04T11:49:41.227454Z","shell.execute_reply":"2025-12-04T11:49:41.789714Z"}},"outputs":[{"name":"stdout","text":"\n✅ Submission file 'submission.csv' created successfully!\n   Shape: (300000, 2)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ... (Previous code for loading data and combining into df_all)\nfrom sklearn.preprocessing import LabelEncoder\n# ...\n\n# --- CRITICAL FEATURE ENGINEERING: Zero-Imputation ---\n# The columns in YOUR dataset where 0 is suspicious or requires careful handling\nzero_cols = ['bmi', 'systolic_bp', 'diastolic_bp', \n             'alcohol_consumption_per_week', 'physical_activity_minutes_per_week']\n\nfor col in zero_cols:\n    # 1. Create a Missing Indicator feature for original zero values\n    # This helps the model distinguish between an imputed value and a genuine measurement\n    df_all[f'{col}_is_zero'] = (df_all[col] == 0).astype(int)\n    \n    # 2. Replace the suspicious 0s with NaN for proper median imputation\n    # Note: We replace 0s with NaN only for 'bmi', 'systolic_bp', 'diastolic_bp'\n    if col in ['bmi', 'systolic_bp', 'diastolic_bp']:\n        df_all[col] = df_all[col].replace(0, np.nan)\n\n# 3. Impute NaN values with the median (best for robustness)\nfor col in ['bmi', 'systolic_bp', 'diastolic_bp']:\n    median_val = df_all[col].median()\n    df_all[col] = df_all[col].fillna(median_val)\n    print(f\"Imputed {col} NaNs with median: {median_val:.2f}\")\n\n\n# --- SECONDARY FEATURE ENGINEERING: Interactions & Ratios ---\n# 4. Create Interaction Features with YOUR column names\ndf_all['BMI_WHR_Ratio'] = df_all['bmi'] / df_all['waist_to_hip_ratio']\ndf_all['Mean_BP'] = (df_all['systolic_bp'] + df_all['diastolic_bp']) / 2\ndf_all['Cholesterol_Ratio'] = df_all['ldl_cholesterol'] / df_all['hdl_cholesterol']\n\n\n# --- 2. Simple Preprocessing: Handle Categorical Features (Adjusted) ---\n# Identify categorical columns (object dtype)\ncat_cols = df_all.select_dtypes(include=['object']).columns\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    # Fill any remaining NaN in categorical features before encoding\n    df_all[col] = df_all[col].astype(str).fillna('missing')\n    le.fit(df_all[col])\n    df_all[col] = le.transform(df_all[col])\n    df_all[col] = df_all[col].astype('category')\n\n# Split back into training and test sets\nX = df_all.iloc[:len(train_df)].drop('id', axis=1)\nX_test = df_all.iloc[len(train_df):].drop('id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:55:36.572043Z","iopub.execute_input":"2025-12-04T11:55:36.572747Z","iopub.status.idle":"2025-12-04T11:55:38.224412Z","shell.execute_reply.started":"2025-12-04T11:55:36.572722Z","shell.execute_reply":"2025-12-04T11:55:38.223628Z"}},"outputs":[{"name":"stdout","text":"Imputed bmi NaNs with median: 25.90\nImputed systolic_bp NaNs with median: 116.00\nImputed diastolic_bp NaNs with median: 75.00\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# --- 4. Optimized Model Configuration (LightGBM with GPU) ---\nlgb_params_optimized = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 10000,          # Increased max capacity\n    'learning_rate': 0.008,         # Slower, more accurate learning\n    'num_leaves': 95,               # Increased complexity\n    'max_depth': -1,\n    'min_child_samples': 10,        # Allows for finer splits\n    'colsample_bytree': 0.7,\n    'subsample': 0.7,\n    'scale_pos_weight': 0.604,      # Handles class imbalance (38% Neg / 62% Pos)\n    'random_state': 42,\n    \n    # --- GPU Configuration ---\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    # -------------------------\n}\n\n# --- 5. Stratified K-Fold Cross-Validation & Training ---\nFOLDS = 5 \nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\ncv_scores = []\n\nprint(\"\\n--- Starting Optimized LightGBM Training (Expected AUC >= 0.75) ---\")\n\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(f\"\\n⚡️ Fold {fold+1}/{FOLDS}\")\n    \n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    model = lgb.LGBMClassifier(**lgb_params_optimized)\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='auc',\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=100)] # Increased stopping rounds for stability\n    )\n\n    val_preds = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_index] = val_preds\n    \n    fold_auc = roc_auc_score(y_val, val_preds)\n    cv_scores.append(fold_auc)\n    print(f\"Fold {fold+1} AUC: {fold_auc:.5f}\")\n\n    test_preds += model.predict_proba(X_test)[:, 1] / FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:57:38.959437Z","iopub.execute_input":"2025-12-04T11:57:38.960146Z","iopub.status.idle":"2025-12-04T12:26:45.091041Z","shell.execute_reply.started":"2025-12-04T11:57:38.960119Z","shell.execute_reply":"2025-12-04T12:26:45.090322Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Optimized LightGBM Training (Expected AUC >= 0.75) ---\n\n⚡️ Fold 1/5\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 2259\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 27\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 22 dense feature groups (12.82 MB) transferred to GPU in 0.015845 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 300 rounds\nEarly stopping, best iteration is:\n[5145]\tvalid_0's auc: 0.727873\nFold 1 AUC: 0.72787\n\n⚡️ Fold 2/5\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 2260\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 27\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 22 dense feature groups (12.82 MB) transferred to GPU in 0.015100 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 300 rounds\nEarly stopping, best iteration is:\n[5615]\tvalid_0's auc: 0.725961\nFold 2 AUC: 0.72596\n\n⚡️ Fold 3/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 2261\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 27\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 22 dense feature groups (12.82 MB) transferred to GPU in 0.015257 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 300 rounds\nEarly stopping, best iteration is:\n[5536]\tvalid_0's auc: 0.726921\nFold 3 AUC: 0.72692\n\n⚡️ Fold 4/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 2258\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 27\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 22 dense feature groups (12.82 MB) transferred to GPU in 0.015349 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 300 rounds\nEarly stopping, best iteration is:\n[4049]\tvalid_0's auc: 0.727682\nFold 4 AUC: 0.72768\n\n⚡️ Fold 5/5\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 2258\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 27\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 22 dense feature groups (12.82 MB) transferred to GPU in 0.015658 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 300 rounds\nEarly stopping, best iteration is:\n[5340]\tvalid_0's auc: 0.727934\nFold 5 AUC: 0.72793\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- 6. Evaluate and Submit ---\nmean_auc = np.mean(cv_scores)\nprint(\"\\n\" + \"=\"*40)\nprint(f\"FINAL OVERALL OOF AUC: {mean_auc:.5f}\")\nprint(\"=\"*40)\n\n# Create the submission file\nsubmission_df = pd.DataFrame({'id': test_df['id'], TARGET: test_preds})\nsubmission_file_path = \"submission_optimized_final.csv\"\nsubmission_df.to_csv(submission_file_path, index=False)\n\nprint(f\"\\n✅ Submission file '{submission_file_path}' created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:26:45.662083Z","iopub.execute_input":"2025-12-04T12:26:45.662274Z","iopub.status.idle":"2025-12-04T12:26:46.233203Z","shell.execute_reply.started":"2025-12-04T12:26:45.662250Z","shell.execute_reply":"2025-12-04T12:26:46.232447Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nFINAL OVERALL OOF AUC: 0.72727\n========================================\n\n✅ Submission file 'submission_optimized_final.csv' created successfully!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}